{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "252dd459-636d-4d96-95f0-01ab705703c4",
   "metadata": {},
   "source": [
    "### Saving data along contours\n",
    "\n",
    "Contours defined in make_contour.ipynb\n",
    "\n",
    "Extract volume transport, dzu and pot_rho_1 along contours\n",
    "\n",
    "Then bin into sigma_1 bins\n",
    "\n",
    "Alternative: save salt and temp, and turn salt and temp into pot_rho_x\n",
    "\n",
    "Each submitted as PBS gadi scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d62330e-adc8-470b-a338-f3706bcce7c7",
   "metadata": {},
   "source": [
    "### Save along contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ab584-ee6b-4a96-be65-c2a7ce7d53bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates transport, rho1 and dzu across contour looped over years for one Southern Ocean contour\n",
    "\"\"\"\n",
    "\n",
    "# Load modules\n",
    "\n",
    "# Standard modules\n",
    "import cosima_cookbook as cc\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dask.distributed import Client\n",
    "import cftime\n",
    "import glob\n",
    "import dask.array as dsa\n",
    "from cosima_cookbook import distributed as ccd\n",
    "# Ignore warnings\n",
    "import logging\n",
    "logging.captureWarnings(True)\n",
    "logging.getLogger('py.warnings').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Start a dask cluster with multiple cores\n",
    "    client = Client(n_workers=8, local_directory='/scratch/x77/cy8964/dask_dump/dask_worker_space')\n",
    "    # Load database\n",
    "    session = cc.database.create_session('/g/data/ik11/databases/cosima_master.db')\n",
    "\n",
    "    #### get run count argument that was passed to python script ####\n",
    "    import sys\n",
    "    year = str(sys.argv[1])\n",
    "    expt = '01deg_jra55v13_ryf9091'\n",
    "\n",
    "    start_time= year + '-01-01'\n",
    "    end_time= year + '-12-31'\n",
    "    contour_no = int(sys.argv[2])\n",
    "    # reference density value:\n",
    "    rho_0 = 1035.0\n",
    "    # Note: change this range, so it matches the size of your contour arrays:\n",
    "    ## FULL SO ##\n",
    "#         lat_range = slice(-70,-29.99)\n",
    "#         lat_range_big =  slice(-70.05,-29.9)\n",
    "    \n",
    "    lat_range = [slice(-60,-34.99),slice(-60,-34.99),slice(-60,-34.99),slice(-60,-34.99),slice(-60,-34.99),\n",
    "                 slice(-60,-39.98),slice(-60,-39.98),slice(-62.91,-45),slice(-62.91,-45),slice(-62.91,-45),\n",
    "                 slice(-64.99,-47),slice(-64.99,-47),slice(-70,-47),slice(-70,-47)][contour_no]\n",
    "    lat_range_big = [slice(-60.05,-34.90),slice(-60.05,-34.90),slice(-60.05,-34.90),slice(-60.05,-34.90),slice(-60.05,-34.90),\n",
    "                     slice(-60.05,-39.90),slice(-60.05,-39.90),slice(-62.96,-44.90),slice(-62.96,-44.90),slice(-62.96,-44.90),\n",
    "                     slice(-65.02,-46.93),slice(-65.02,-46.93),slice(-70.05,-46.93),slice(-70.05,-46.93)][contour_no]\n",
    "    # t-cells are further south and west than u-cells\n",
    "    ## some grid data is required, a little complicated because these variables don't behave well with some\n",
    "    dyt = cc.querying.getvar(expt, 'dyt',session, n=1, ncfile = 'ocean_grid.nc')\n",
    "    dxu = cc.querying.getvar(expt, 'dxu',session, n=1, ncfile = 'ocean_grid.nc')\n",
    "\n",
    "    # select latitude range:\n",
    "    dxu = dxu.sel(yu_ocean=lat_range)\n",
    "    dyt = dyt.sel(yt_ocean=lat_range)\n",
    "\n",
    "    SSH = [-0.1,-0.2,-0.3,-0.4,-0.5,-0.6,-0.7,-0.8,-0.9,-1.0,-1.1,-1.2,-1.3,-1.4][contour_no]\n",
    "\n",
    "    SO_SSH = 'SO_slope_contour_'+str(SSH)+'m_SSH.npz'\n",
    "\n",
    "    outfile = '/g/data/x77/cy8964/Post_Process/'+SO_SSH\n",
    "\n",
    "    choicename = ['SO_A','SO_B','SO_C','SO_D','SO_E','SO_F','SO_G','SO_H','SO_I','SO_J','SO_K','SO_L','SO_M','SO_N'][contour_no]\n",
    "\n",
    "\n",
    "    # t-cells are further south and west than u-cells\n",
    "    ## some grid data is required, a little complicated because these variables don't behave well with some\n",
    "    dyt = cc.querying.getvar(expt, 'dyt',session, n=1, ncfile = 'ocean_grid.nc')\n",
    "    dxu = cc.querying.getvar(expt, 'dxu',session, n=1, ncfile = 'ocean_grid.nc')\n",
    "\n",
    "    # select latitude range:\n",
    "    dxu = dxu.sel(yu_ocean=lat_range)\n",
    "    dyt = dyt.sel(yt_ocean=lat_range)\n",
    "\n",
    "    data = np.load(outfile)\n",
    "    mask_y_transport = data['mask_y_transport']\n",
    "    mask_x_transport = data['mask_x_transport']\n",
    "    mask_y_transport_numbered = data['mask_y_transport_numbered']\n",
    "    mask_x_transport_numbered = data['mask_x_transport_numbered']\n",
    "\n",
    "#         #pad masks to help with interpolation\n",
    "#         mask_x_transport = np.pad(mask_x_transport, ((0, 0), (1, 1)),'constant', constant_values=((0,0), (0,0)))\n",
    "#         mask_y_transport = np.pad(mask_y_transport, ((0, 0), (1, 1)),'constant', constant_values=((0,0), (0,0)))\n",
    "#         mask_x_transport_numbered = np.pad(mask_x_transport_numbered, ((0, 0), (1, 1)),'constant', constant_values=((0,0), (0,0)))\n",
    "#         mask_y_transport_numbered = np.pad(mask_y_transport_numbered, ((0, 0), (1, 1)),'constant', constant_values=((0,0), (0,0)))\n",
    "\n",
    "\n",
    "    yt_ocean = cc.querying.getvar(expt,'yt_ocean',session,n=1)\n",
    "    yt_ocean = yt_ocean.sel(yt_ocean=lat_range)\n",
    "    yu_ocean = cc.querying.getvar(expt,'yu_ocean',session,n=1)\n",
    "    yu_ocean = yu_ocean.sel(yu_ocean=lat_range)\n",
    "    xt_ocean = cc.querying.getvar(expt,'xt_ocean',session,n=1)\n",
    "    xu_ocean = cc.querying.getvar(expt,'xu_ocean',session,n=1)\n",
    "#         xt_ocean = xt_ocean.sel(xt_ocean = lon_range_big)\n",
    "#         xu_ocean = xu_ocean.sel(xu_ocean = lon_range_big)\n",
    "\n",
    "    # Convert contour masks to data arrays, so we can multiply them later.\n",
    "    # We need to ensure the lat lon coordinates correspond to the actual data location:\n",
    "    #       The y masks are used for vhrho, so like vhrho this should have dimensions (yu_ocean, xt_ocean).\n",
    "    #       The x masks are used for uhrho, so like uhrho this should have dimensions (yt_ocean, xu_ocean).\n",
    "    #       However the actual name will always be simply y_ocean/x_ocean irrespective of the variable\n",
    "    #       to make concatenation of transports in both direction and sorting possible.\n",
    "\n",
    "    mask_x_transport = xr.DataArray(mask_x_transport, coords = [('y_ocean', yt_ocean), ('x_ocean', xu_ocean)])\n",
    "    mask_y_transport = xr.DataArray(mask_y_transport, coords = [('y_ocean', yu_ocean), ('x_ocean', xt_ocean)])\n",
    "    mask_x_transport_numbered = xr.DataArray(mask_x_transport_numbered, coords = [('y_ocean', yt_ocean), ('x_ocean', xu_ocean)])\n",
    "    mask_y_transport_numbered = xr.DataArray(mask_y_transport_numbered, coords = [('y_ocean', yu_ocean), ('x_ocean', xt_ocean)])\n",
    "    # Create the contour order data-array. Note that in this procedure the x-grid counts have x-grid\n",
    "    #   dimensions and the y-grid counts have y-grid dimensions, but these are implicit, the dimension\n",
    "    #   *names* are kept general across the counts, the generic y_ocean, x_ocean, so that concatening works\n",
    "    #   but we dont double up with numerous counts for one lat/lon point.\n",
    "\n",
    "    # stack contour data into 1d:\n",
    "    mask_x_numbered_1d = mask_x_transport_numbered.stack(contour_index = ['y_ocean', 'x_ocean'])\n",
    "    mask_x_numbered_1d = mask_x_numbered_1d.where(mask_x_numbered_1d > 0, drop = True)\n",
    "    mask_y_numbered_1d = mask_y_transport_numbered.stack(contour_index = ['y_ocean', 'x_ocean'])\n",
    "    mask_y_numbered_1d = mask_y_numbered_1d.where(mask_y_numbered_1d > 0, drop = True)\n",
    "    contour_ordering = xr.concat((mask_x_numbered_1d,mask_y_numbered_1d), dim = 'contour_index')\n",
    "    contour_ordering = contour_ordering.sortby(contour_ordering)\n",
    "    contour_index_array = np.arange(1,len(contour_ordering)+1)\n",
    "\n",
    "    # Note vhrho_nt is v*dz*1035 and is positioned on north centre edge of t-cell.\n",
    "    vhrho = cc.querying.getvar(expt,'vhrho_nt',session,start_time=start_time, end_time=end_time)\n",
    "    uhrho = cc.querying.getvar(expt,'uhrho_et',session,start_time=start_time, end_time=end_time)\n",
    "\n",
    "    # select latitude range and this month:\n",
    "    vhrho = vhrho.sel(yt_ocean=lat_range).sel(time=slice(start_time,end_time))\n",
    "    uhrho = uhrho.sel(yt_ocean=lat_range).sel(time=slice(start_time,end_time))\n",
    "\n",
    "    # Note that vhrho is defined as the transport across the northern edge of a tracer cell so its coordinates\n",
    "    #       should be (yu_ocean, xt_ocean).\n",
    "    #  uhrho is defined as the transport across the eastern edge of a tracer cell so its coordinates should\n",
    "    #       be (yt_ocean, xu_ocean).\n",
    "    #  However we will keep the actual name as simply y_ocean/x_ocean irrespective of the variable\n",
    "    #       to make concatenation and sorting possible.\n",
    "    yt_ocean = dyt.yt_ocean.values\n",
    "    yu_ocean = dxu.yu_ocean.values\n",
    "    xu_ocean = dxu.xu_ocean.values\n",
    "    xt_ocean = dyt.xt_ocean.values\n",
    "    vhrho.coords['yt_ocean'] = yu_ocean\n",
    "    uhrho.coords['xt_ocean'] = xu_ocean\n",
    "    vhrho = vhrho.rename({'yt_ocean':'y_ocean', 'xt_ocean':'x_ocean'})\n",
    "    uhrho = uhrho.rename({'yt_ocean':'y_ocean', 'xt_ocean':'x_ocean'})\n",
    "\n",
    "    # First we also need to change coords on dxu, dyt, so we can multiply the transports:\n",
    "    dyt = dyt.reset_coords().dyt # remove geolon_t/geolat_t coordinates\n",
    "    dxu = dxu.reset_coords().dxu # remove geolon_t/geolat_t coordinates\n",
    "    dxu.coords['xu_ocean'] = xt_ocean\n",
    "    dxu = dxu.rename({'yu_ocean':'y_ocean', 'xu_ocean':'x_ocean'})\n",
    "    dyt.coords['xt_ocean'] = xu_ocean\n",
    "    dyt = dyt.rename({'yt_ocean':'y_ocean','xt_ocean':'x_ocean'})\n",
    "\n",
    "    # convert to transports and multiply by contour masks:\n",
    "    vhrho = vhrho*dxu*mask_y_transport/rho_0\n",
    "    uhrho = uhrho*dyt*mask_x_transport/rho_0\n",
    "\n",
    "    ## initiate a empty dataarray\n",
    "    vol_trans_across_contour = xr.DataArray(np.zeros((len(uhrho.time),len(uhrho.st_ocean),len(contour_index_array))),\n",
    "                                        coords = [uhrho.time,uhrho.st_ocean, contour_index_array],\n",
    "                                        dims = ['time','st_ocean', 'contour_index'],\n",
    "                                        name = 'vol_trans_across_contour')\n",
    "\n",
    "    for time_step in range(len(uhrho.time)):\n",
    "        print(time_step)\n",
    "        # load one timestep of transport data:\n",
    "        # loading here speeds it up a lot:\n",
    "        uhrho_i = uhrho[time_step,...]\n",
    "        uhrho_i = uhrho_i.fillna(0)\n",
    "        uhrho_i = uhrho_i.load()\n",
    "        vhrho_i = vhrho[time_step,...]\n",
    "        vhrho_i = vhrho_i.fillna(0)\n",
    "        vhrho_i = vhrho_i.load()\n",
    "\n",
    "        # stack transports into 1d and drop any points not on contour:\n",
    "        x_transport_1d_i = uhrho_i.stack(contour_index = ['y_ocean', 'x_ocean'])\n",
    "        x_transport_1d_i = x_transport_1d_i.where(mask_x_numbered_1d>0, drop = True)\n",
    "        y_transport_1d_i = vhrho_i.stack(contour_index = ['y_ocean', 'x_ocean'])\n",
    "        y_transport_1d_i = y_transport_1d_i.where(mask_y_numbered_1d>0, drop = True)\n",
    "\n",
    "        # combine all points on contour:\n",
    "        vol_trans_across_contour_i = xr.concat((x_transport_1d_i, y_transport_1d_i), dim = 'contour_index')\n",
    "        vol_trans_across_contour_i = vol_trans_across_contour_i.sortby(contour_ordering)\n",
    "        vol_trans_across_contour_i.coords['contour_index'] = contour_index_array\n",
    "        vol_trans_across_contour_i = vol_trans_across_contour_i.load()\n",
    "\n",
    "        # write into larger array:\n",
    "        vol_trans_across_contour[time_step,:,:] = vol_trans_across_contour_i\n",
    "\n",
    "        del uhrho_i, vhrho_i, x_transport_1d_i, y_transport_1d_i, vol_trans_across_contour_i\n",
    "\n",
    "    save_dir = '/g/data/x77/cy8964/Post_Process/New_SO/'+choicename+'_'\n",
    "    ds_vol_trans_across_contour = xr.Dataset({'vol_trans_across_contour': vol_trans_across_contour})\n",
    "    ds_vol_trans_across_contour.to_netcdf(save_dir+'vol_trans_across_contour_'+year+'.nc')\n",
    "\n",
    "    ########## now save dzu for this year ###############\n",
    "    yt_ocean = cc.querying.getvar(expt,'yt_ocean',session,n=1)\n",
    "    yt_ocean = yt_ocean.sel(yt_ocean=lat_range)\n",
    "    yu_ocean = cc.querying.getvar(expt,'yu_ocean',session,n=1)\n",
    "    yu_ocean = yu_ocean.sel(yu_ocean=lat_range)\n",
    "    xt_ocean = cc.querying.getvar(expt,'xt_ocean',session,n=1)\n",
    "    xu_ocean = cc.querying.getvar(expt,'xu_ocean',session,n=1)\n",
    "#         xt_ocean = xt_ocean.sel(xt_ocean = lon_range_big)\n",
    "#         xu_ocean = xu_ocean.sel(xu_ocean = lon_range_big)\n",
    "\n",
    "    dzt = cc.querying.getvar(expt,'dzt',session,start_time=start_time, end_time=end_time,ncfile='%daily%')\n",
    "    dzt = dzt.sel(yt_ocean=lat_range).sel(time=slice(start_time,end_time))\n",
    "\n",
    "    # inititalise empty array\n",
    "    dzu_along_contour = xr.DataArray(np.zeros((len(dzt.time),len(dzt.st_ocean),len(contour_index_array))),\n",
    "                                      coords = [dzt.time,dzt.st_ocean, contour_index_array],\n",
    "                                      dims = ['time','st_ocean', 'contour_index'],\n",
    "                                      name = 'dzu_along_contour')\n",
    "\n",
    "    for time_step in range(len(dzt.time)):\n",
    "        print(time_step)\n",
    "    #     if time_step == 3:\n",
    "    #         break\n",
    "\n",
    "        # This is faster if we load first here:\n",
    "        dzt_i = dzt[time_step,...]\n",
    "        dzt_i = dzt_i.fillna(0)\n",
    "        dzt_i = dzt_i.load()\n",
    "        dzt_i = dzt_i.rename({'yt_ocean':'y_ocean', 'xt_ocean':'x_ocean'})\n",
    "\n",
    "        # Note that this interpolation does not work as generically as e.g. salt.interp(),\n",
    "        #    but it is much faster and doesn't require removing chunking (which also slow things down).\n",
    "        # Be careful that your latitude range extends at least one point either direction beyond your contour.\n",
    "        # If your domain is not the full longitude range, you will need to adapt this, so you have the correct interpolation\n",
    "        #    only the edges of your domain (it assumes it is reentrant).\n",
    "        # Need to overwrite coords, so these two variables can be added together.\n",
    "\n",
    "        #First create dzu\n",
    "\n",
    "        dzt_i_right = dzt_i.roll(x_ocean = -1, roll_coords = False)\n",
    "        dzt_i_up = dzt_i.roll(y_ocean = -1,roll_coords = False)\n",
    "        dzt_i_up_right=dzt_i.roll(y_ocean = -1,roll_coords = False).roll(x_ocean = -1, roll_coords = False)\n",
    "        dzu = np.fmin(np.fmin(np.fmin(dzt_i,dzt_i_right),dzt_i_up),dzt_i_up_right)\n",
    "\n",
    "        #now the xgrid needs BAY(dzu) while ygrid needs BAX(dzu) so that they are on uhrho and vhrho grids\n",
    "\n",
    "        dzu_n = dzu.copy()\n",
    "        dzu_s = dzu.roll(y_ocean = 1, roll_coords=False)\n",
    "        BAY_dzu = (dzu_n+dzu_s)/2\n",
    "        BAY_dzu['x_ocean'] = xu_ocean.values\n",
    "\n",
    "        dzu_e = dzu.copy()\n",
    "        dzu_w = dzu.roll(x_ocean = 1, roll_coords=False)\n",
    "        BAX_dzu = (dzu_w+dzu_e)/2\n",
    "        BAX_dzu['y_ocean'] = yu_ocean.values\n",
    "\n",
    "        # stack transports into 1d and drop any points not on contour:\n",
    "        BAY_dzu = BAY_dzu.where(mask_x_transport_numbered>0)\n",
    "        BAX_dzu = BAX_dzu.where(mask_y_transport_numbered>0)\n",
    "        x_dzu_1d = BAY_dzu.stack(contour_index = ['y_ocean', 'x_ocean'])\n",
    "        y_dzu_1d = BAX_dzu.stack(contour_index = ['y_ocean', 'x_ocean'])\n",
    "        x_dzu_1d = x_dzu_1d.where(mask_x_numbered_1d>0,drop=True)\n",
    "        y_dzu_1d = y_dzu_1d.where(mask_y_numbered_1d>0,drop=True)\n",
    "\n",
    "        # combine all points on contour:\n",
    "        dzu_along_contour_i = xr.concat((x_dzu_1d, y_dzu_1d), dim = 'contour_index')\n",
    "        dzu_along_contour_i = dzu_along_contour_i.sortby(contour_ordering)\n",
    "        dzu_along_contour_i.coords['contour_index'] = contour_index_array\n",
    "        dzu_along_contour_i = dzu_along_contour_i.load()\n",
    "\n",
    "        # write into larger array:\n",
    "        dzu_along_contour[time_step,:,:] = dzu_along_contour_i\n",
    "\n",
    "        del dzt_i,dzu,dzu_w,dzu_e, dzu_s, dzu_n, BAY_dzu, BAX_dzu, x_dzu_1d, y_dzu_1d, dzu_along_contour_i\n",
    "\n",
    "    ### Save:\n",
    "    save_dir = '/g/data/x77/cy8964/Post_Process/New_SO/'+choicename+'_'\n",
    "\n",
    "    ds_dzu_along_contour = xr.Dataset({'dzu_along_contour': dzu_along_contour})\n",
    "    ds_dzu_along_contour.to_netcdf(save_dir+'dzu_along_contour_'+year+'.nc')\n",
    "\n",
    "    ########## now save pot_rho_1 for this year ##########\n",
    "    yt_ocean = cc.querying.getvar(expt,'yt_ocean',session,n=1)\n",
    "    yt_ocean = yt_ocean.sel(yt_ocean=lat_range)\n",
    "    yu_ocean = cc.querying.getvar(expt,'yu_ocean',session,n=1)\n",
    "    yu_ocean = yu_ocean.sel(yu_ocean=lat_range)\n",
    "    yu_ocean = yu_ocean.rename({'yu_ocean':'y_ocean'})\n",
    "    xt_ocean = cc.querying.getvar(expt,'xt_ocean',session,n=1)\n",
    "    xu_ocean = cc.querying.getvar(expt,'xu_ocean',session,n=1)\n",
    "#         xt_ocean = xt_ocean.sel(xt_ocean = lon_range_big)\n",
    "#         xu_ocean = xu_ocean.sel(xu_ocean = lon_range_big)\n",
    "    xu_ocean = xu_ocean.rename({'xu_ocean':'x_ocean'})\n",
    "\n",
    "    rho = cc.querying.getvar(expt,'pot_rho_1',session,start_time=start_time, end_time=end_time,ncfile='%daily%')\n",
    "    rho = rho.sel(yt_ocean=lat_range).sel(time=slice(start_time,end_time))\n",
    "    # inititalise empty array\n",
    "    rho_along_contour = xr.DataArray(np.zeros((len(rho.time),len(rho.st_ocean),len(contour_index_array))),\n",
    "                                      coords = [rho.time,rho.st_ocean, contour_index_array],\n",
    "                                      dims = ['time','st_ocean', 'contour_index'],\n",
    "                                      name = 'rho_along_contour')\n",
    "\n",
    "    for time_step in range(len(rho.time)):\n",
    "        print(time_step)\n",
    "    #     if time_step == 5:\n",
    "    #         break\n",
    "\n",
    "        # This is faster if we load first here:\n",
    "        rho_i = rho[time_step,...]\n",
    "        rho_i = rho_i.fillna(0)\n",
    "        rho_i = rho_i.load()\n",
    "        rho_i = rho_i.rename({'yt_ocean':'y_ocean', 'xt_ocean':'x_ocean'})\n",
    "\n",
    "        # Note that this interpolation does not work as generically as e.g. rho.interp(),\n",
    "        #    but it is much faster and doesn't require removing chunking (which also slow things down).\n",
    "        # Be careful that your latitude range extends at least one point either direction beyond your contour.\n",
    "        # If your domain is not the full longitude range, you will need to adapt this, so you have the correct interpolation\n",
    "        #    only the edges of your domain (it assumes it is reentrant).\n",
    "        # Need to overwrite coords, so these two variables can be added together:\n",
    "        rho_w = rho_i.copy()\n",
    "        rho_w.coords['x_ocean'] = xu_ocean.values\n",
    "        rho_e = rho_i.roll(x_ocean=-1)\n",
    "        rho_e.coords['x_ocean'] = xu_ocean.values\n",
    "        # rho_xgrid will be on the uhrho grid:\n",
    "        rho_xgrid = (rho_e + rho_w)/2\n",
    "\n",
    "        rho_s = rho_i.copy()\n",
    "        rho_s.coords['y_ocean'] = yu_ocean.values\n",
    "        rho_n = rho_i.roll(y_ocean=-1)\n",
    "        rho_n.coords['y_ocean'] = yu_ocean.values\n",
    "        # rho_ygrid will be on the vhrho grid:\n",
    "        rho_ygrid = (rho_s + rho_n)/2\n",
    "\n",
    "        # stack transports into 1d and drop any points not on contour:\n",
    "        rho_xgrid = rho_xgrid.where(mask_x_transport_numbered>0)\n",
    "        rho_ygrid = rho_ygrid.where(mask_y_transport_numbered>0)\n",
    "        x_rho_1d = rho_xgrid.stack(contour_index = ['y_ocean', 'x_ocean'])\n",
    "        y_rho_1d = rho_ygrid.stack(contour_index = ['y_ocean', 'x_ocean'])\n",
    "        x_rho_1d = x_rho_1d.where(mask_x_numbered_1d>0,drop=True)\n",
    "        y_rho_1d = y_rho_1d.where(mask_y_numbered_1d>0,drop=True)\n",
    "\n",
    "        # combine all points on contour:\n",
    "        rho_along_contour_i = xr.concat((x_rho_1d, y_rho_1d), dim = 'contour_index')\n",
    "        rho_along_contour_i = rho_along_contour_i.sortby(contour_ordering)\n",
    "        rho_along_contour_i.coords['contour_index'] = contour_index_array\n",
    "        rho_along_contour_i = rho_along_contour_i.load()\n",
    "\n",
    "        # write into larger array:\n",
    "        rho_along_contour[time_step,:,:] = rho_along_contour_i\n",
    "\n",
    "        del rho_i,rho_w,rho_e, rho_s, rho_n, rho_xgrid, rho_ygrid, x_rho_1d, y_rho_1d, rho_along_contour_i\n",
    "\n",
    "    ### Save:\n",
    "    save_dir = '/g/data/x77/cy8964/Post_Process/New_SO/'+choicename+'_'\n",
    "\n",
    "    ds_rho_along_contour = xr.Dataset({'pot_rho_1_along_contour': rho_along_contour})\n",
    "    ds_rho_along_contour.to_netcdf(save_dir+'pot_rho_1_along_contour_'+year+'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e2b1d-3009-4ff3-8872-046ec2850676",
   "metadata": {},
   "source": [
    "### Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d65ac76-19d4-4a87-b133-036da26734c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bins volume transport and dzu for 10 years for one Southern Ocean contour\n",
    "\"\"\"\n",
    "\n",
    "# Load modules\n",
    "\n",
    "# Standard modules\n",
    "import cosima_cookbook as cc\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dask.distributed import Client\n",
    "import cftime\n",
    "import glob\n",
    "import dask.array as dsa\n",
    "from cosima_cookbook import distributed as ccd\n",
    "# Ignore warnings\n",
    "import logging\n",
    "logging.captureWarnings(True)\n",
    "logging.getLogger('py.warnings').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Start a dask cluster with multiple cores\n",
    "    client = Client(n_workers=8, local_directory='/scratch/x77/cy8964/dask_dump/dask_worker_space')\n",
    "    # Load database\n",
    "    session = cc.database.create_session('/g/data/ik11/databases/cosima_master.db')\n",
    "\n",
    "    #### get run count argument that was passed to python script ####\n",
    "    import sys\n",
    "    contour_no = int(sys.argv[1]) ## this is range 0 to 13, defining which contour (SSH=-0.1 to -1.4)\n",
    "\n",
    "    expt = '01deg_jra55v13_ryf9091'\n",
    "    year = '2170'\n",
    "    start_time= year + '-01-01'\n",
    "    end_time= year + '-12-31'\n",
    "\n",
    "    # reference density value:\n",
    "    rho_0 = 1035.0\n",
    "    # Note: change this range, so it matches the size of your contour arrays:\n",
    "    # Note different contours have different ranges for efficiency. Ensure the same range as how contours were made\n",
    "    lat_range = [slice(-60,-34.99),slice(-60,-34.99),slice(-60,-34.99),slice(-60,-34.99),slice(-60,-34.99),\n",
    "                 slice(-60,-39.98),slice(-60,-39.98),slice(-62.91,-45),slice(-62.91,-45),slice(-62.91,-45),\n",
    "                 slice(-64.99,-47),slice(-64.99,-47),slice(-70,-47),slice(-70,-47)][contour_no]\n",
    "    lat_range_big = [slice(-60.05,-34.90),slice(-60.05,-34.90),slice(-60.05,-34.90),slice(-60.05,-34.90),slice(-60.05,-34.90),\n",
    "                     slice(-60.05,-39.90),slice(-60.05,-39.90),slice(-62.96,-44.90),slice(-62.96,-44.90),slice(-62.96,-44.90),\n",
    "                     slice(-65.02,-46.93),slice(-65.02,-46.93),slice(-70.05,-46.93),slice(-70.05,-46.93)][contour_no]\n",
    "    # t-cells are further south and west than u-cells\n",
    "    ## some grid data is required, a little complicated because these variables don't behave well with some\n",
    "    dyt = cc.querying.getvar(expt, 'dyt',session, n=1, ncfile = 'ocean_grid.nc')\n",
    "    dxu = cc.querying.getvar(expt, 'dxu',session, n=1, ncfile = 'ocean_grid.nc')\n",
    "\n",
    "    # select latitude range:\n",
    "    dxu = dxu.sel(yu_ocean=lat_range)\n",
    "    dyt = dyt.sel(yt_ocean=lat_range)\n",
    "\n",
    "    SSH = [-0.1,-0.2,-0.3,-0.4,-0.5,-0.6,-0.7,-0.8,-0.9,-1.0,-1.1,-1.2,-1.3,-1.4][contour_no]\n",
    "\n",
    "    SO_SSH = 'SO_slope_contour_'+str(SSH)+'m_SSH.npz'\n",
    "\n",
    "    outfile = '/g/data/x77/cy8964/Post_Process/'+SO_SSH\n",
    "\n",
    "    choicename = ['SO_A','SO_B','SO_C','SO_D','SO_E','SO_F','SO_G','SO_H','SO_I','SO_J','SO_K','SO_L','SO_M','SO_N'][contour_no]\n",
    "\n",
    "    data = np.load(outfile)\n",
    "    mask_y_transport = data['mask_y_transport']\n",
    "    mask_x_transport = data['mask_x_transport']\n",
    "    mask_y_transport_numbered = data['mask_y_transport_numbered']\n",
    "    mask_x_transport_numbered = data['mask_x_transport_numbered']\n",
    "\n",
    "#         #pad masks to help with interpolation\n",
    "#         mask_x_transport = np.pad(mask_x_transport, ((0, 0), (1, 1)),'constant', constant_values=((0,0), (0,0)))\n",
    "#         mask_y_transport = np.pad(mask_y_transport, ((0, 0), (1, 1)),'constant', constant_values=((0,0), (0,0)))\n",
    "#         mask_x_transport_numbered = np.pad(mask_x_transport_numbered, ((0, 0), (1, 1)),'constant', constant_values=((0,0), (0,0)))\n",
    "#         mask_y_transport_numbered = np.pad(mask_y_transport_numbered, ((0, 0), (1, 1)),'constant', constant_values=((0,0), (0,0)))\n",
    "\n",
    "\n",
    "    yt_ocean = cc.querying.getvar(expt,'yt_ocean',session,n=1)\n",
    "    yt_ocean = yt_ocean.sel(yt_ocean=lat_range)\n",
    "    yu_ocean = cc.querying.getvar(expt,'yu_ocean',session,n=1)\n",
    "    yu_ocean = yu_ocean.sel(yu_ocean=lat_range)\n",
    "    xt_ocean = cc.querying.getvar(expt,'xt_ocean',session,n=1)\n",
    "    xu_ocean = cc.querying.getvar(expt,'xu_ocean',session,n=1)\n",
    "#         xt_ocean = xt_ocean.sel(xt_ocean = lon_range_big)\n",
    "#         xu_ocean = xu_ocean.sel(xu_ocean = lon_range_big)\n",
    "\n",
    "    # Convert contour masks to data arrays, so we can multiply them later.\n",
    "    # We need to ensure the lat lon coordinates correspond to the actual data location:\n",
    "    #       The y masks are used for vhrho, so like vhrho this should have dimensions (yu_ocean, xt_ocean).\n",
    "    #       The x masks are used for uhrho, so like uhrho this should have dimensions (yt_ocean, xu_ocean).\n",
    "    #       However the actual name will always be simply y_ocean/x_ocean irrespective of the variable\n",
    "    #       to make concatenation of transports in both direction and sorting possible.\n",
    "\n",
    "    mask_x_transport = xr.DataArray(mask_x_transport, coords = [('y_ocean', yt_ocean), ('x_ocean', xu_ocean)])\n",
    "    mask_y_transport = xr.DataArray(mask_y_transport, coords = [('y_ocean', yu_ocean), ('x_ocean', xt_ocean)])\n",
    "    mask_x_transport_numbered = xr.DataArray(mask_x_transport_numbered, coords = [('y_ocean', yt_ocean), ('x_ocean', xu_ocean)])\n",
    "    mask_y_transport_numbered = xr.DataArray(mask_y_transport_numbered, coords = [('y_ocean', yu_ocean), ('x_ocean', xt_ocean)])\n",
    "    # Create the contour order data-array. Note that in this procedure the x-grid counts have x-grid\n",
    "    #   dimensions and the y-grid counts have y-grid dimensions, but these are implicit, the dimension\n",
    "    #   *names* are kept general across the counts, the generic y_ocean, x_ocean, so that concatening works\n",
    "    #   but we dont double up with numerous counts for one lat/lon point.\n",
    "\n",
    "    # stack contour data into 1d:\n",
    "    mask_x_numbered_1d = mask_x_transport_numbered.stack(contour_index = ['y_ocean', 'x_ocean'])\n",
    "    mask_x_numbered_1d = mask_x_numbered_1d.where(mask_x_numbered_1d > 0, drop = True)\n",
    "    mask_y_numbered_1d = mask_y_transport_numbered.stack(contour_index = ['y_ocean', 'x_ocean'])\n",
    "    mask_y_numbered_1d = mask_y_numbered_1d.where(mask_y_numbered_1d > 0, drop = True)\n",
    "    contour_ordering = xr.concat((mask_x_numbered_1d,mask_y_numbered_1d), dim = 'contour_index')\n",
    "    contour_ordering = contour_ordering.sortby(contour_ordering)\n",
    "    contour_index_array = np.arange(1,len(contour_ordering)+1)\n",
    "\n",
    "\n",
    "    ########## now bin dzu for all 10 years ##########\n",
    "    yt_ocean = cc.querying.getvar(expt,'yt_ocean',session,n=1)\n",
    "    yt_ocean = yt_ocean.sel(yt_ocean=lat_range)\n",
    "    yu_ocean = cc.querying.getvar(expt,'yu_ocean',session,n=1)\n",
    "    yu_ocean = yu_ocean.sel(yu_ocean=lat_range)\n",
    "    yu_ocean = yu_ocean.rename({'yu_ocean':'y_ocean'})\n",
    "    xt_ocean = cc.querying.getvar(expt,'xt_ocean',session,n=1)\n",
    "    xu_ocean = cc.querying.getvar(expt,'xu_ocean',session,n=1)\n",
    "    xu_ocean = xu_ocean.rename({'xu_ocean':'x_ocean'})\n",
    "    \n",
    "    # get lat and lon along contour, useful for plotting later:\n",
    "    lat_along_contour = contour_ordering.y_ocean\n",
    "    lon_along_contour = contour_ordering.x_ocean\n",
    "    contour_index_array = np.arange(1,len(contour_ordering)+1)\n",
    "    # don't need the multi-index anymore, replace with contour count and save\n",
    "    lat_along_contour.coords['contour_index'] = contour_index_array\n",
    "    lon_along_contour.coords['contour_index'] = contour_index_array\n",
    "\n",
    "    # extract saved dzu and sigma1 along contours, which are saved for each year\n",
    "    year = '2170'\n",
    "    save_dir = '/g/data/x77/cy8964/Post_Process/New_SO/'+choicename+'_'\n",
    "\n",
    "    vol_trans_across_contour = xr.open_dataset(save_dir+'dzu_along_contour_'+year+'.nc')\n",
    "    vol_trans_across_contour = vol_trans_across_contour.dzu_along_contour\n",
    "    vol_trans_across_contour = vol_trans_across_contour.load()\n",
    "\n",
    "    sigma1_along_contour = xr.open_dataset(save_dir+'pot_rho_1_along_contour_'+year+'.nc')\n",
    "    sigma1_along_contour = sigma1_along_contour.pot_rho_1_along_contour\n",
    "    sigma1_along_contour = sigma1_along_contour.load()\n",
    "    for year in np.arange(2171,2180):\n",
    "\n",
    "        vol_trans_across_contour_i = xr.open_dataset(save_dir+'dzu_along_contour_'+str(year)+'.nc')\n",
    "        vol_trans_across_contour_i = vol_trans_across_contour_i.dzu_along_contour\n",
    "        vol_trans_across_contour_i = vol_trans_across_contour_i.load()\n",
    "\n",
    "        sigma1_along_contour_i = xr.open_dataset(save_dir+'pot_rho_1_along_contour_'+str(year)+'.nc')\n",
    "        sigma1_along_contour_i = sigma1_along_contour_i.pot_rho_1_along_contour\n",
    "        sigma1_along_contour_i = sigma1_along_contour_i.load()\n",
    "\n",
    "        vol_trans_across_contour = xr.concat([vol_trans_across_contour,vol_trans_across_contour_i], dim = 'time')\n",
    "        sigma1_along_contour = xr.concat([sigma1_along_contour,sigma1_along_contour_i], dim = 'time')\n",
    "\n",
    "\n",
    "    time = sigma1_along_contour.time\n",
    "\n",
    "    ## define isopycnal bins\n",
    "    isopycnal_bins_sigma1 = 1000+ np.array([1,28,29,30,31,31.5,31.9,32,32.1,32.2,32.25,\n",
    "                                                32.3,32.35,32.4,32.42,32.44,32.46,32.48,32.50,32.51,\n",
    "                                                32.52,32.53,32.54,32.55,32.56,32.58,32.6,32.8,33,34,45])\n",
    "\n",
    "\n",
    "    ## intialise empty transport along contour in density bins array\n",
    "    vol_trans_across_contour_binned = xr.DataArray(np.zeros((len(time),len(isopycnal_bins_sigma1),len(contour_ordering))), \n",
    "                                                   coords = [time,isopycnal_bins_sigma1, contour_index_array], \n",
    "                                                   dims = ['time','isopycnal_bins', 'contour_index'], \n",
    "                                                   name = 'vol_trans_across_contour_binned')\n",
    "\n",
    "    # loop through density bins:\n",
    "    for i in range(len(isopycnal_bins_sigma1)-1):\n",
    "        print(i)\n",
    "        bin_mask = sigma1_along_contour.where(sigma1_along_contour<=isopycnal_bins_sigma1[i+1]).where(sigma1_along_contour>isopycnal_bins_sigma1[i])*0+1\n",
    "        bin_fractions = (isopycnal_bins_sigma1[i+1]-sigma1_along_contour * bin_mask)/(isopycnal_bins_sigma1[i+1]-isopycnal_bins_sigma1[i])\n",
    "        ## transport\n",
    "        transport_across_contour_in_sigmalower_bin = (vol_trans_across_contour * bin_mask * bin_fractions).sum(dim = 'st_ocean')\n",
    "        vol_trans_across_contour_binned[:,i,:] += transport_across_contour_in_sigmalower_bin.fillna(0)\n",
    "        del transport_across_contour_in_sigmalower_bin\n",
    "        transport_across_contour_in_sigmaupper_bin = (vol_trans_across_contour * bin_mask * (1-bin_fractions)).sum(dim = 'st_ocean')\n",
    "        vol_trans_across_contour_binned[:,i+1,:] += transport_across_contour_in_sigmaupper_bin.fillna(0)\n",
    "        del bin_mask, bin_fractions, transport_across_contour_in_sigmaupper_bin\n",
    "\n",
    "    #save\n",
    "    year = '2170'\n",
    "    save_dir = '/g/data/x77/cy8964/Post_Process/New_SO/'+choicename+'_'\n",
    "\n",
    "    ds_vol_trans_across_contour_binned = xr.Dataset({'dzu_across_contour_binned': vol_trans_across_contour_binned})\n",
    "    ds_vol_trans_across_contour_binned.to_netcdf(save_dir+'dzu_across_contour_binned.nc')\n",
    "\n",
    "    year = '2170'\n",
    "    save_dir = '/g/data/x77/cy8964/Post_Process/New_SO/'+choicename+'_'\n",
    "\n",
    "    #bin volume\n",
    "    # extract volume and sigma1 along contours for each year\n",
    "    vol_trans_across_contour = xr.open_dataset(save_dir+'vol_trans_across_contour_'+year+'.nc')\n",
    "    vol_trans_across_contour = vol_trans_across_contour.vol_trans_across_contour\n",
    "    vol_trans_across_contour = vol_trans_across_contour.chunk(chunks = {\"time\":1,\"st_ocean\":10,\"contour_index\":200})\n",
    "    vol_trans_across_contour = vol_trans_across_contour.load()\n",
    "\n",
    "\n",
    "    sigma1_along_contour = xr.open_dataset(save_dir+'pot_rho_1_along_contour_'+year+'.nc')\n",
    "    sigma1_along_contour = sigma1_along_contour.pot_rho_1_along_contour\n",
    "    sigma1_along_contour = sigma1_along_contour.chunk(chunks = {\"time\":1,\"st_ocean\":10,\"contour_index\":200})\n",
    "    sigma1_along_contour = sigma1_along_contour.load()\n",
    "    for year in np.arange(2171,2180):\n",
    "        vol_trans_across_contour_i = xr.open_dataset(save_dir+'vol_trans_across_contour_'+str(year)+'.nc')\n",
    "        vol_trans_across_contour_i = vol_trans_across_contour_i.vol_trans_across_contour\n",
    "        vol_trans_across_contour_i = vol_trans_across_contour_i.chunk(chunks = {\"time\":1,\"st_ocean\":10,\"contour_index\":200 })\n",
    "        vol_trans_across_contour_i = vol_trans_across_contour_i.load()\n",
    "\n",
    "        sigma1_along_contour_i = xr.open_dataset(save_dir+'pot_rho_1_along_contour_'+str(year)+'.nc')\n",
    "        sigma1_along_contour_i = sigma1_along_contour_i.pot_rho_1_along_contour\n",
    "        sigma1_along_contour_i = sigma1_along_contour_i.chunk(chunks = {\"time\":1,\"st_ocean\":10,\"contour_index\":200 })\n",
    "        sigma1_along_contour_i = sigma1_along_contour_i.load()\n",
    "\n",
    "        vol_trans_across_contour = xr.concat([vol_trans_across_contour,vol_trans_across_contour_i], dim = 'time')\n",
    "        sigma1_along_contour = xr.concat([sigma1_along_contour,sigma1_along_contour_i], dim = 'time')\n",
    "\n",
    "\n",
    "    time = sigma1_along_contour.time\n",
    "\n",
    "    ## define isopycnal bins\n",
    "    isopycnal_bins_sigma1 = 1000+ np.array([1,28,29,30,31,31.5,31.9,32,32.1,32.2,32.25,\n",
    "                                                32.3,32.35,32.4,32.42,32.44,32.46,32.48,32.50,32.51,\n",
    "                                                32.52,32.53,32.54,32.55,32.56,32.58,32.6,32.8,33,34,45])\n",
    "\n",
    "\n",
    "    ## intialise empty transport along contour in density bins array\n",
    "    vol_trans_across_contour_binned = xr.DataArray(np.zeros((len(time),len(isopycnal_bins_sigma1),len(contour_ordering))), \n",
    "                                                   coords = [time,isopycnal_bins_sigma1, contour_index_array], \n",
    "                                                   dims = ['time','isopycnal_bins', 'contour_index'], \n",
    "                                                   name = 'vol_trans_across_contour_binned')\n",
    "\n",
    "    # loop through density bins:\n",
    "    for i in range(len(isopycnal_bins_sigma1)-1):\n",
    "        print(i)\n",
    "        bin_mask = sigma1_along_contour.where(sigma1_along_contour<=isopycnal_bins_sigma1[i+1]).where(sigma1_along_contour>isopycnal_bins_sigma1[i])*0+1\n",
    "        bin_fractions = (isopycnal_bins_sigma1[i+1]-sigma1_along_contour * bin_mask)/(isopycnal_bins_sigma1[i+1]-isopycnal_bins_sigma1[i])\n",
    "        ## transport\n",
    "        transport_across_contour_in_sigmalower_bin = (vol_trans_across_contour * bin_mask * bin_fractions).sum(dim = 'st_ocean')\n",
    "        vol_trans_across_contour_binned[:,i,:] += transport_across_contour_in_sigmalower_bin.fillna(0)\n",
    "        del transport_across_contour_in_sigmalower_bin\n",
    "        transport_across_contour_in_sigmaupper_bin = (vol_trans_across_contour * bin_mask * (1-bin_fractions)).sum(dim = 'st_ocean')\n",
    "        vol_trans_across_contour_binned[:,i+1,:] += transport_across_contour_in_sigmaupper_bin.fillna(0)\n",
    "        del bin_mask, bin_fractions, transport_across_contour_in_sigmaupper_bin\n",
    "\n",
    "    year = '2170'\n",
    "    save_dir = '/g/data/x77/cy8964/Post_Process/New_SO/'+choicename+'_' \n",
    "\n",
    "    ds_vol_trans_across_contour_binned = xr.Dataset({'vol_trans_across_contour_binned': vol_trans_across_contour_binned})\n",
    "    ds_vol_trans_across_contour_binned.to_netcdf(save_dir+'vol_trans_across_contour_binned.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af9d993-ee6c-4c8a-a2d0-4faf8cafec31",
   "metadata": {},
   "source": [
    "### If saved salt and temp instead (swap pot_rho_1 variable for salt and temp):\n",
    "you can get potential density from\n",
    "\n",
    "from gsw import SA_from_SP, p_from_z, sigma2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137f9f7b-7ca2-4b97-9197-c9f8708d363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    temp_along_contour = xr.open_dataset(save_dir+'temp_along_contour_'+year+'.nc')\n",
    "    temp_along_contour = temp_along_contour.temp_along_contour\n",
    "    temp_along_contour = temp_along_contour.load()\n",
    "    \n",
    "    salt_along_contour = xr.open_dataset(save_dir+'salt_along_contour_'+year+'.nc')\n",
    "    salt_along_contour = salt_along_contour.salt_along_contour\n",
    "    salt_along_contour = salt_along_contour.load()    \n",
    "    # load the other years too to have a 10 year daily dataset\n",
    "    for year in np.arange(2171,2180):\n",
    "\n",
    "#         dzu_along_contour_i = xr.open_dataset(save_dir+'dzu_along_contour_'+str(year)+'.nc')\n",
    "#         dzu_along_contour_i = dzu_along_contour_i.dzu_along_contour\n",
    "#         dzu_along_contour_i = dzu_along_contour_i.load()\n",
    "\n",
    "        temp_along_contour_i = xr.open_dataset(save_dir+'temp_along_contour_'+str(year)+'.nc')\n",
    "        temp_along_contour_i = temp_along_contour_i.temp_along_contour\n",
    "        temp_along_contour_i = temp_along_contour_i.load()\n",
    "\n",
    "        salt_along_contour_i = xr.open_dataset(save_dir+'salt_along_contour_'+str(year)+'.nc')\n",
    "        salt_along_contour_i = salt_along_contour_i.salt_along_contour\n",
    "        salt_along_contour_i = salt_along_contour_i.load()    \n",
    "\n",
    "\n",
    "#         dzu_along_contour = xr.concat([dzu_along_contour,dzu_along_contour_i], dim = 'time')\n",
    "        temp_along_contour = xr.concat([temp_along_contour,temp_along_contour_i], dim = 'time')\n",
    "        salt_along_contour = xr.concat([salt_along_contour,salt_along_contour_i], dim = 'time')\n",
    "\n",
    "    # turn salt and temp into sigma2\n",
    "    time = salt_along_contour.time\n",
    "    st_ocean = cc.querying.getvar(expt,'st_ocean',session,n=1)\n",
    "    depth = -st_ocean.values\n",
    "    depth = xr.DataArray(depth, coords = [st_ocean], dims = ['st_ocean'])\n",
    "    depth_along_contour = (salt_along_contour[0,...]*0+1)*depth\n",
    "\n",
    "    pressure_along_contour = xr.DataArray(p_from_z(depth_along_contour,lat_along_contour), \n",
    "                                          coords = [st_ocean, contour_index_array], \n",
    "                                          dims = ['st_ocean','contour_index'], \n",
    "                                          name = 'pressure', attrs = {'units':'dbar'})\n",
    "\n",
    "    # absolute salinity:\n",
    "    abs_salt_along_contour = xr.DataArray(SA_from_SP(salt_along_contour,pressure_along_contour,\n",
    "                                                 lon_along_contour,lat_along_contour), \n",
    "                                      coords = [time,st_ocean,contour_index_array], \n",
    "                                      dims = ['time','st_ocean','contour_index'], \n",
    "                                      name = 'Absolute salinity', \n",
    "                                      attrs = {'units':'Absolute Salinity (g/kg)'})\n",
    "    # sigma2:\n",
    "    sigma2_along_contour = xr.DataArray(sigma2(abs_salt_along_contour, temp_along_contour-273.15),\n",
    "                                         coords = [time,st_ocean, contour_index_array], \n",
    "                                        dims = ['time','st_ocean', 'contour_index'], \n",
    "                                         name = 'potential density ref 2000dbar', \n",
    "                                        attrs = {'units':'kg/m^3 (-1000 kg/m^3)'})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-21.04]",
   "language": "python",
   "name": "conda-env-analysis3-21.04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
