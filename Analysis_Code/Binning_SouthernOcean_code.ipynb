{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "726ba4b8-f026-4565-a369-84c04cf58335",
   "metadata": {},
   "source": [
    "### Code to bin Southern Ocean into sigma_1 density bins\n",
    "\n",
    "Submitted as a PBS script on gadi with year and month input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e8133-e550-4391-ad44-925ee3ee7381",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculates vh,uh and h binned into density bins in Southern Ocean for 1 month\n",
    "\"\"\"\n",
    "\n",
    "# Load modules\n",
    "\n",
    "# Standard modules\n",
    "import cosima_cookbook as cc\n",
    "import matplotlib.pyplot as plt\n",
    "import netCDF4 as nc\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from dask.distributed import Client\n",
    "import cftime\n",
    "import glob\n",
    "import dask.array as dsa\n",
    "from cosima_cookbook import distributed as ccd\n",
    "# Ignore warnings\n",
    "import logging\n",
    "logging.captureWarnings(True)\n",
    "logging.getLogger('py.warnings').setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Start a dask cluster with multiple cores\n",
    "    client = Client(n_workers=8, local_directory='/scratch/x77/cy8964/dask_dump/dask_worker_space')\n",
    "    # Load database\n",
    "    session = cc.database.create_session('/g/data/ik11/databases/cosima_master.db')\n",
    "\n",
    "    # This script just calculates binned quantities for a single year.\n",
    "\n",
    "    \n",
    "    #### get run count argument that was passed to python script ####\n",
    "    import sys\n",
    "    run_count = int(sys.argv[1])\n",
    "    \n",
    "    if run_count<10:\n",
    "        month = '0'+str(run_count)\n",
    "    elif run_count ==10:\n",
    "        month = '10'\n",
    "    elif run_count ==11:\n",
    "        month = '11'\n",
    "    elif run_count ==12:\n",
    "        month = '12'\n",
    "    # load time slices\n",
    "    #year_i = int(sys.argv[2])\n",
    "    year = str(sys.argv[2])\n",
    "    expt = '01deg_jra55v13_ryf9091'\n",
    "\n",
    "    time_slice= year + '-' + month\n",
    "    start_time = year + '-01-01'\n",
    "    end_time= year + '-12-31'\n",
    "\n",
    "    # reference density value:\n",
    "    rho_0 = 1035.0\n",
    "    # Note: change this range, so it matches the size of your contour arrays:\n",
    "    ## FULL SO ##\n",
    "    lat_range = slice(-70,-34.99)\n",
    "    lat_range_big =  slice(-70.05,-34.90)\n",
    "\n",
    "    #load coordinates\n",
    "    yt_ocean = cc.querying.getvar(expt,'yt_ocean',session,n=1)\n",
    "    yt_ocean = yt_ocean.sel(yt_ocean=lat_range)\n",
    "    xt_ocean = cc.querying.getvar(expt,'xt_ocean',session,n=1)\n",
    "\n",
    "    #load vhrho and uhrho\n",
    "    # Note vhrho_nt is v*dz*1035 and is positioned on north centre edge of t-cell.\n",
    "    vhrho = cc.querying.getvar(expt,'vhrho_nt',session,start_time=start_time, end_time=end_time)\n",
    "    uhrho = cc.querying.getvar(expt,'uhrho_et',session,start_time=start_time, end_time=end_time)\n",
    "\n",
    "    # select latitude range and this month:\n",
    "    vhrho = vhrho.sel(yt_ocean=lat_range_big).sel(time=time_slice)\n",
    "    uhrho = uhrho.sel(yt_ocean=lat_range).sel(time=time_slice)\n",
    "\n",
    "    uhrho_E = uhrho[:,:,:,3599]\n",
    "    uhrho_W = uhrho[:,:,:,0]\n",
    "    uhrho_E.xt_ocean.values = -280.05\n",
    "    uhrho_W.xt_ocean.values = 80.05\n",
    "    uhrho = xr.concat([uhrho_E, uhrho, uhrho_W], dim = 'xt_ocean')\n",
    "\n",
    "    #load pot_rho_1\n",
    "    pot_rho_1 = cc.querying.getvar(expt,'pot_rho_1',session,start_time=start_time, end_time=end_time,ncfile='%daily%')\n",
    "    pot_rho_1 = pot_rho_1.sel(yt_ocean=lat_range).sel(time=time_slice)\n",
    "    time = pot_rho_1.time\n",
    "\n",
    "    #load dzt\n",
    "    dzt = cc.querying.getvar(expt,'dzt',session,start_time=start_time, end_time=end_time,ncfile='%daily%')\n",
    "    dzt = dzt.sel(yt_ocean=lat_range_big).sel(time=time_slice)\n",
    "\n",
    "    ## define isopycnal bins\n",
    "    isopycnal_bins_sigma1 = 1000+ np.array([1,28,29,30,31,31.5,31.9,32,32.1,32.2,32.25,\n",
    "                                                32.3,32.35,32.4,32.42,32.44,32.46,32.48,32.50,32.51,\n",
    "                                                32.52,32.53,32.54,32.55,32.56,32.58,32.6,32.8,33,34,45])\n",
    "    ## intialise empty transport along contour in density bins array\n",
    "    vh_binned = xr.DataArray(np.zeros((len(time),len(isopycnal_bins_sigma1),len(yt_ocean),len(xt_ocean))),\n",
    "                                                   coords = [time,isopycnal_bins_sigma1, yt_ocean, xt_ocean],\n",
    "                                                   dims = ['time','isopycnal_bins', 'yt_ocean','xt_ocean'],\n",
    "                                                   name = 'vh_binned')\n",
    "    uh_binned = xr.DataArray(np.zeros((len(time),len(isopycnal_bins_sigma1),len(yt_ocean),len(xt_ocean))),\n",
    "                                                   coords = [time,isopycnal_bins_sigma1, yt_ocean, xt_ocean],\n",
    "                                                   dims = ['time','isopycnal_bins', 'yt_ocean','xt_ocean'],\n",
    "                                                   name = 'uh_binned')\n",
    "    h_binned = xr.DataArray(np.zeros((len(time),len(isopycnal_bins_sigma1),len(yt_ocean),len(xt_ocean))),\n",
    "                                                   coords = [time,isopycnal_bins_sigma1, yt_ocean, xt_ocean],\n",
    "                                                   dims = ['time','isopycnal_bins', 'yt_ocean','xt_ocean'],\n",
    "                                                   name = 'h_binned')\n",
    "    # loop over time for that month\n",
    "    for day in range(len(time)):\n",
    "        print('day '+str(day))\n",
    "\n",
    "        uhrho_j = uhrho[day,...]\n",
    "        uhrho_j = uhrho_j.fillna(0)\n",
    "        uhrho_j = uhrho_j.load()\n",
    "        vhrho_j = vhrho[day,...]\n",
    "        vhrho_j = vhrho_j.fillna(0)\n",
    "        vhrho_j = vhrho_j.load()\n",
    "\n",
    "        #move uhrho and vhrho to t grid since uhrho is eastern side of cell and vhrho on northern\n",
    "        uhrho_t = 0.5*(uhrho_j+uhrho_j.roll(xt_ocean=1, roll_coords = False)) #this takes average of adjacent cells\n",
    "        uhrho_t = uhrho_t[:,:,1:-1]\n",
    "\n",
    "        vhrho_t = 0.5*(vhrho_j+vhrho_j.roll(yt_ocean=1, roll_coords = False))\n",
    "        vhrho_t = vhrho_t[:,1:-1,:]\n",
    "        #load dzt and pot_rho_1 for the \"week\"\n",
    "        dzt_j = dzt[day,...]\n",
    "        dzt_j = dzt_j.fillna(0)\n",
    "        dzt_j = dzt_j.load()\n",
    "        \n",
    "        #vhrho and uhrho grids are from BAY(dzu) and BAX(dzu) NOT dzt: find these\n",
    "        dzt_j_right = dzt_j.roll(xt_ocean = -1, roll_coords = False)\n",
    "        dzt_j_up = dzt_j.roll(yt_ocean = -1,roll_coords = False)\n",
    "        dzt_j_up_right=dzt_j.roll(yt_ocean = -1,roll_coords = False).roll(xt_ocean = -1, roll_coords = False)\n",
    "        dzu = np.fmin(np.fmin(np.fmin(dzt_j,dzt_j_right),dzt_j_up),dzt_j_up_right)\n",
    "        #now the xgrid needs BAY(dzu) while ygrid needs BAX(dzu) so that they are on uhrho and vhrho grids. we only need to do one,\n",
    "        #as they are equivalent when interpolatred to t-grid. I chose to find BAX because I took dzt to have bigger lon range\n",
    "        dzu_e = dzu.copy()\n",
    "        dzu_w = dzu.roll(xt_ocean = 1, roll_coords=False)\n",
    "        BAX_dzu = (dzu_w+dzu_e)/2    \n",
    "\n",
    "        #as with vhrho these need to be moved to t-grid in the same way:\n",
    "        dzt_j = 0.5*(BAX_dzu+BAX_dzu.roll(yt_ocean=1, roll_coords = False))\n",
    "        dzt_j = dzt_j[:,1:-1,:]\n",
    "        \n",
    "        pot_rho_1_j = pot_rho_1[day,...]\n",
    "        pot_rho_1_j = pot_rho_1_j.fillna(0)\n",
    "        pot_rho_1_j = pot_rho_1_j.load()\n",
    "\n",
    "        # now bin into density bins\n",
    "        for i in range(len(isopycnal_bins_sigma1)-1):\n",
    "            print(i)\n",
    "            #create masks for isopycnal binnning, that are 1 where the density that day is between two bin values, and 0 elsewhere\n",
    "            bin_mask = pot_rho_1_j.where(pot_rho_1_j<=isopycnal_bins_sigma1[i+1]).where(pot_rho_1_j>isopycnal_bins_sigma1[i])*0+1\n",
    "            # create a fractional value that splits the transport between each bin based on which bin it is closer to\n",
    "            bin_fractions = (isopycnal_bins_sigma1[i+1]-pot_rho_1_j * bin_mask)/(isopycnal_bins_sigma1[i+1]-isopycnal_bins_sigma1[i])\n",
    "\n",
    "            ## vh - splits transport between the two bins and saves into vh_binned array\n",
    "            transport_across_contour_in_sigmalower_bin = ( vhrho_t * bin_mask * bin_fractions).sum(dim = 'st_ocean')\n",
    "            vh_binned[day,i,:,:] += transport_across_contour_in_sigmalower_bin.fillna(0)\n",
    "            del transport_across_contour_in_sigmalower_bin\n",
    "            transport_across_contour_in_sigmaupper_bin = ( vhrho_t * bin_mask * (1-bin_fractions)).sum(dim = 'st_ocean')\n",
    "            vh_binned[day,i+1,:,:] += transport_across_contour_in_sigmaupper_bin.fillna(0)\n",
    "            del transport_across_contour_in_sigmaupper_bin\n",
    "\n",
    "            ## uh\n",
    "            transport_across_contour_in_sigmalower_bin = ( uhrho_t * bin_mask * bin_fractions).sum(dim = 'st_ocean')\n",
    "            uh_binned[day,i,:,:] += transport_across_contour_in_sigmalower_bin.fillna(0)\n",
    "            del transport_across_contour_in_sigmalower_bin\n",
    "            transport_across_contour_in_sigmaupper_bin = ( uhrho_t * bin_mask * (1-bin_fractions)).sum(dim = 'st_ocean')\n",
    "            uh_binned[day,i+1,:,:] += transport_across_contour_in_sigmaupper_bin.fillna(0)\n",
    "            del transport_across_contour_in_sigmaupper_bin\n",
    "\n",
    "            ## h\n",
    "            transport_across_contour_in_sigmalower_bin = (dzt_j * bin_mask * bin_fractions).sum(dim = 'st_ocean')\n",
    "            h_binned[day,i,:,:] += transport_across_contour_in_sigmalower_bin.fillna(0)\n",
    "            del transport_across_contour_in_sigmalower_bin\n",
    "            transport_across_contour_in_sigmaupper_bin = (dzt_j * bin_mask * (1-bin_fractions)).sum(dim = 'st_ocean')\n",
    "            h_binned[day,i+1,:,:] += transport_across_contour_in_sigmaupper_bin.fillna(0)\n",
    "            del bin_mask, bin_fractions, transport_across_contour_in_sigmaupper_bin\n",
    "\n",
    "        del pot_rho_1_j, dzt_j, uhrho_j, vhrho_j, uhrho_t, vhrho_t, dzt_j_right,dzt_j_up, dzt_j_up_right, BAX_dzu, dzu_e, dzu_w\n",
    "\n",
    "    #save monthly data\n",
    "    save_dir = '/g/data/x77/cy8964/Post_Process/New_SO/SO_saving_energy_terms_'\n",
    "\n",
    "    ds_vol_trans_across_contour_binned = xr.Dataset({'vh_binned': vh_binned})\n",
    "    ds_vol_trans_across_contour_binned.to_netcdf(save_dir+'vh_binned_'+year+'-'+month+'_test.nc',\n",
    "                                                 encoding={'vh_binned': {'shuffle': True, 'zlib': True, 'complevel': 5}})\n",
    "\n",
    "    ds_vol_trans_across_contour_binned = xr.Dataset({'uh_binned': uh_binned})\n",
    "    ds_vol_trans_across_contour_binned.to_netcdf(save_dir+'uh_binned_'+year+'-'+month+'_test.nc',\n",
    "                                                 encoding={'uh_binned': {'shuffle': True, 'zlib': True, 'complevel': 5}})\n",
    "\n",
    "    ds_vol_trans_across_contour_binned = xr.Dataset({'h_binned': h_binned})\n",
    "    ds_vol_trans_across_contour_binned.to_netcdf(save_dir+'h_binned_'+year+'-'+month+'_test.nc',\n",
    "                                                 encoding={'h_binned': {'shuffle': True, 'zlib': True, 'complevel': 5}})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:analysis3-21.04]",
   "language": "python",
   "name": "conda-env-analysis3-21.04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
